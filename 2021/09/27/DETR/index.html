<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Qingren Yao"><title>DETR:End-to-End Object Detection with Transformers · Qingren Yao</title><meta name="description" content="MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$"><meta name="keywords" content="Hexo,HTML,CSS,android,Linux"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">Qingren Yao</a></h3><div class="description"><p>Active Open-source Developer, Electric Info Master.</p></div></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://twitter.com/Qingren_2000"><i class="fa fa-twitter fa-2x"></i></a></li><li><a target="_blank" rel="noopener" href="http://github.com/qingrenn"><i class="fa fa-github fa-2x"></i></a></li><li><a href="mailto:yaoqingrenrobin@gmail.com"><i class="fa fa-envelope fa-2x"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>@ Qingren Yao 2022</span></a></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/archives">Archive</a></li><li><a href="/about">About</a></li><li><a href="/timeline">Timeline</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>DETR:End-to-End Object Detection with Transformers</a></h3></div><div class="post-content"><head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

<h2 id="DETR"><a href="#DETR" class="headerlink" title="DETR"></a>DETR</h2><p>End-to-End Object Detection with Transformers [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/detr">源码</a>] [<a target="_blank" rel="noopener" href="https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers">论文</a>]</p>
<p>DETR是一种基于Transformers的端到端的目标检测模型。</p>
<p>简化了Faster-RCNN、YOLO系列使用proposal-classifier的目标识别流程。</p>
<p>DETR的模型结构非常简单，下图把DETR的所有<strong>关键部分</strong>全部体现了出来：</p>
<ul>
<li>Backbone</li>
<li>Embedding(positional_embeding, query_embeding)</li>
<li>Encoder</li>
<li>Decoder</li>
<li>Feed Forward Network</li>
</ul>
<center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/截屏2021-09-26 下午8.50.50.png"></center>

<p>先对模型的前馈流程，以及各个模块的详细组成进行简单介绍。</p>
<span id="more"></span>
<p>首先，在图像输入网络前，是需要对其进行尺寸调整的。因为在transformer推理的过程中，需要batch中所有样本的序列长度都一样，所以要把batch的所有图像调整到统一的尺寸。这里会对图像做padding，然后会通过一个mask来记录调整后图像中被padding的位置。mask尺寸为$(bs,H,W)$。</p>
<h3 id="1-Backbone"><a href="#1-Backbone" class="headerlink" title="1. Backbone"></a>1. Backbone</h3><p>输入一组图像，张量维度为$(bs,C,H,W)$。</p>
<p>Backbone使用的是Resnet，在经过特征提取后第四个block的输出，分辨率下降16倍，通道数为2048。</p>
<p>将Backbone的输出记为$(bs,2048,h,w)$。</p>
<h3 id="2-Positional-Encoding"><a href="#2-Positional-Encoding" class="headerlink" title="2.Positional Encoding"></a>2.Positional Encoding</h3><p>源码中提供了两种Positional Encoding的方式，这也是目前两种主流的编码方式。</p>
<p>一种使用正余弦编码（相对位置编码），一种可学习的编码方式（绝对位置编码），两种编码方式在最后的结果中并无太大差异，接下来分别对其进行介绍。</p>
<h4 id="2-1-Sinusoidal-Position-Encoding"><a href="#2-1-Sinusoidal-Position-Encoding" class="headerlink" title="2.1 Sinusoidal Position Encoding"></a>2.1 Sinusoidal Position Encoding</h4><p>在介绍二维的正余弦编码前，可以先回顾一下Attention is all you need 中的一维的正余弦编码。</p>
<p>其编码规则是这样的：</p>
<p>$PE_(pos,2i)=sin(pos/10000^{2i/d_{model}})$</p>
<p>$PE_(pos,2i+1)=cos(pos/10000^{2i/d_{model}})$</p>
<p>其中$pos$表示单词所处句子中的第$pos$个位置，$d_{model}$表示词向量的维度，$i$表示这个词向量的第i维。示例可见上一篇<a href="https://qingrenn.github.io/2021/08/20/Attention%20is%20all%20you%20need/#more">博客</a></p>
<p>而对于二维的正余弦编码，实际上就是单独对x方向，和单独对y方向编码，然后concat。</p>
<p>接下来通过代码详细解读一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建位置编码器，关键参数 N_steps = hidden_dim // 2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_position_encoding</span>(<span class="params">args</span>):</span></span><br><span class="line">    N_steps = args.hidden_dim // <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> args.position_embedding <span class="keyword">in</span> (<span class="string">&#x27;v2&#x27;</span>, <span class="string">&#x27;sine&#x27;</span>):</span><br><span class="line">        <span class="comment"># TODO find a better way of exposing other arguments</span></span><br><span class="line">        position_embedding = PositionEmbeddingSine(N_steps, normalize=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">elif</span> args.position_embedding <span class="keyword">in</span> (<span class="string">&#x27;v3&#x27;</span>, <span class="string">&#x27;learned&#x27;</span>):</span><br><span class="line">        position_embedding = PositionEmbeddingLearned(N_steps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">f&quot;not supported <span class="subst">&#123;args.position_embedding&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> position_embedding</span><br></pre></td></tr></table></figure>
<p>有一个关键参数<code>N_steps = hidden_dim // 2</code>，实际上hidden_dim就是输入Transformer中的向量的长度。</p>
<p>因为是将对x和对y方向编码的结果concat到一起，所以这里N_steps就为hidden_dim的一半。</p>
<p>接下来看二维的正余弦编码究竟是如何做的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, tensor_list: NestedTensor</span>):</span></span><br><span class="line">        x = tensor_list.tensors</span><br><span class="line">        mask = tensor_list.mask <span class="comment"># (bs, h, w)</span></span><br><span class="line">        <span class="keyword">assert</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        not_mask = ~mask</span><br><span class="line">        y_embed = not_mask.cumsum(<span class="number">1</span>, dtype=torch.float32) <span class="comment"># 在维度1的方向上累加</span></span><br><span class="line">        x_embed = not_mask.cumsum(<span class="number">2</span>, dtype=torch.float32) <span class="comment"># 在维度2的方向上累加</span></span><br><span class="line">        <span class="keyword">if</span> self.normalize: <span class="comment"># 归一化</span></span><br><span class="line">            eps = <span class="number">1e-6</span></span><br><span class="line">            y_embed = y_embed / (y_embed[:, -<span class="number">1</span>:, :] + eps) * self.scale</span><br><span class="line">            x_embed = x_embed / (x_embed[:, :, -<span class="number">1</span>:] + eps) * self.scale</span><br><span class="line">				</span><br><span class="line">        <span class="comment"># num_pos_feats = N_steps</span></span><br><span class="line">        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device) </span><br><span class="line">        dim_t = self.temperature ** (<span class="number">2</span> * (dim_t // <span class="number">2</span>) / self.num_pos_feats) <span class="comment"># (N_steps, )</span></span><br><span class="line"></span><br><span class="line">        pos_x = x_embed[:, :, :, <span class="literal">None</span>] / dim_t </span><br><span class="line">        pos_y = y_embed[:, :, :, <span class="literal">None</span>] / dim_t <span class="comment"># (bs, h, w, N_steps)</span></span><br><span class="line">        pos_x = torch.stack((pos_x[:, :, :, <span class="number">0</span>::<span class="number">2</span>].sin(), pos_x[:, :, :, <span class="number">1</span>::<span class="number">2</span>].cos()), dim=<span class="number">4</span>).flatten(<span class="number">3</span>) <span class="comment"># (bs, h, w, N_steps)</span></span><br><span class="line">        pos_y = torch.stack((pos_y[:, :, :, <span class="number">0</span>::<span class="number">2</span>].sin(), pos_y[:, :, :, <span class="number">1</span>::<span class="number">2</span>].cos()), dim=<span class="number">4</span>).flatten(<span class="number">3</span>) <span class="comment"># (bs, h, w, N_steps)</span></span><br><span class="line">        pos = torch.cat((pos_y, pos_x), dim=<span class="number">3</span>).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (bs, h, w, 2*N_steps)</span></span><br><span class="line">        <span class="keyword">return</span> pos</span><br></pre></td></tr></table></figure>
<p>直接上图解：</p>
<p>mask $(bs, h , w)$ 中1代表图像padding的位置，0代表的是未经过padding的位置。首先取反的到not_mask，然后在h方向上进行累加，可以的到y_embed，在w方向上进行累加得到x_embed。</p>
<center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/image-20210927103050006.png" width=50%></center>

<p>然后进行归一化（表格中的元素还要再乘以scale，scale为$2\pi$）：</p>
<center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_25670A736FF0-1.jpeg width=60%></center>

<p>再生成一组dim_t，长度为N_steps。</p>
<p>以N_steps为64为例：$\{10000^{0/64},10000^{0/64},10000^{2/64},10000^{2/64},\dots,10000^{62/64},10000^{62/64}\}$</p>
<p>再将embed$(bs,h,w,1)$除上dim_t，得到pos_x，pos_y($bs, h, w, N_{steps}$)，再进行正弦，余弦操作。</p>
<p>假设N_steps=64，以上图的情况为例，此时pos_y中第3行任意一列对应的位置编码为：</p>
<p>$\{sin(\frac{2}{3}10000^{0/64}), cos(\frac{2}{3}10000^{0/64}), sin(\frac{2}{3}10000^{2/64}), cos(\frac{2}{3}10000^{0/64}), \dots, sin(\frac{2}{3}10000^{62/64}),cos(\frac{2}{3}10000^{62/64})\}$</p>
<p>此时pos_x中第3列的第2，3，4行对应的位置编码为：</p>
<p>$\{sin(\frac{3}{5}10000^{0/64}), cos(\frac{3}{5}10000^{0/64}), sin(\frac{3}{5}10000^{2/64}), cos(\frac{3}{5}10000^{0/64}), \dots, sin(\frac{3}{5}10000^{62/64}),cos(\frac{3}{5}10000^{62/64})\}$</p>
<p>最后对pos_x和pos_y在第4维进行concat操作得到$(bs, h, w, 2N_{steps})$。</p>
<p>最后调整通道方便下一步操作$（bs, 2N_{steps},h,w）$</p>
<h4 id="2-2-Learned-Positional-Embedding"><a href="#2-2-Learned-Positional-Embedding" class="headerlink" title="2.2 Learned Positional Embedding"></a>2.2 Learned Positional Embedding</h4><center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/image-20210927111043033.png width=30%></center>

<p>可学习的位置编码相对简单。</p>
<p>图像x尺寸调整后为$(c, h ,w)$，那么直接使用<code>nn.Embedding(50, num_pos_features)</code>对行和列进行随机初始化。</p>
<p>其中<code>num_pos_features = N_steps = hidden_dim // 2</code>。</p>
<p>x_emb：$(w, num_pos_features)$</p>
<p>y_emb：$(h,num_pos_features)$</p>
<p>然后如上图所示，x_emb向h方向复制（红色），y_emb向w方向复制（绿色），得到两个尺寸为$(h, w, num_pos_features)$的张量，再进行concat。</p>
<p>最终得到$(h, w, 2num_pos_features)$。</p>
<p>然后调整通道，方便下一步操作：$(bs, 2num_pos_features,h,w)$。</p>
<h3 id="3-Transformer"><a href="#3-Transformer" class="headerlink" title="3 Transformer"></a>3 Transformer</h3><center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/image-20210927165521987.png width=80%></center>

<p>在数据流向Transformer前， 我们已经有了三份数据：</p>
<ul>
<li>经过backbone提取的特征图：$(bs, c, h, w)$</li>
<li>位置编码：$(bs, hidden_dim, h, w),~hidden_dim=d_model$</li>
<li>图像对应的mask：$(h, w)$</li>
</ul>
<p>还要第四份数据：query_embeding，作为Decoder的输入，这一步放在Decoder介绍。</p>
<p>这里Transformer的整体结构与Attention is all you need中非常相似，这里主要关注Decoder的输入。</p>
<p><strong>前处理</strong>：</p>
<p>Transformer的输入与卷积操作不同，需要对数据的格式进行一些变换。</p>
<p>首先为了降低计算量，减少特征图的通道数：$(bs, 2048, h, w) \to (bs, d_model, h, w) \to (hw, bs, d_model)$</p>
<p>位置编码则也要调整格式：$(bs,d_model,h,w) \to (hw, bs, d_model)$</p>
<p>mask也进行调整：$(h,w)\to(hw, )$</p>
<h4 id="3-1-Encoder-Layer"><a href="#3-1-Encoder-Layer" class="headerlink" title="3.1 Encoder Layer"></a>3.1 Encoder Layer</h4><center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/image-20210927113529342.png" width=80%></center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_post</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                     src,</span></span></span><br><span class="line"><span class="params"><span class="function">                     src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                     src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                     pos: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>):</span></span><br><span class="line">        q = k = self.with_pos_embed(src, pos) <span class="comment"># q = k = src + pos_embed </span></span><br><span class="line">        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,</span><br><span class="line">                              key_padding_mask=src_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        src = src + self.dropout1(src2)</span><br><span class="line">        src = self.norm1(src)</span><br><span class="line">        <span class="comment"># Feed forward</span></span><br><span class="line">        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))</span><br><span class="line">        src = src + self.dropout2(src2)</span><br><span class="line">        src = self.norm2(src)</span><br><span class="line">        <span class="keyword">return</span> src <span class="comment"># (hw, bs , d_model)</span></span><br></pre></td></tr></table></figure>
<center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/r9HFXJ.png width=50%></center>

<p>Encoder输入的是原图的特征图，输出的尺寸仍然保持不变，相当于每个像素的编码经过self-attention后都引入了其他像素点的信息，建立了原图区域之间的联系。</p>
<h4 id="3-2-Decoder-Layer"><a href="#3-2-Decoder-Layer" class="headerlink" title="3.2 Decoder Layer"></a>3.2 Decoder Layer</h4><center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/image-20210927123919440.png width=80%></center>

<p>Decoder第一层的输入是一个初始化的维度为$(num_queries,d_model), num_queries=100$的全零张量作为输入。</p>
<p>然后再经过position_embeding，加上一个<strong>随初始化，可学习</strong>的query_embeding（和初始化的tgt同样尺寸）。</p>
<p>我们知道$Attention(Q,K,V) = Scorefunc(QK)V$</p>
<p>在cross-attention中，$q$来自全零张量，进行通道调整，还加上query_embeding：$(num_queries,bs,d_model)$</p>
<p>$k$来自Encoder的输出加上位置编码：$(hw, bs, d_model)$</p>
<p>$v$来自Encoder的输出：$(hw, bs, d_model)$</p>
<p><strong>通过cross-attention，相当于将$num_queries$个查询，结合encoder输出的特征，得到了$num_queries$个目标。</strong></p>
<p>最后Deocder的输出为$(num_queries, bs, d_model)$</p>
<h3 id="4-FeedForward"><a href="#4-FeedForward" class="headerlink" title="4. FeedForward"></a>4. FeedForward</h3><p>最后接两组并行的全联接层，一组输出每个query对应的得分情况，一组输出每个query对应的bbox。</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2021-09-27</span><i class="fa fa-tag"></i><a class="tag" href="/categories/学习总结/" title="学习总结">学习总结 </a><a class="tag" href="/tags/Transformer/" title="Transformer">Transformer </a><a class="tag" href="/tags/Object-Detection/" title="Object Detection">Object Detection </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://qingrenn.github.io/2021/09/27/DETR/,Qingren Yao,DETR:End-to-End Object Detection with Transformers,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2021/10/04/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/" title="CV入门学习路线">prev_post</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2021/09/22/HSI-Fusion/" title="HSI/MSI Fusion">next_post</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>