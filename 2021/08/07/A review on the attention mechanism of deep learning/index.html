<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Qingren Yao"><title>A review on the attention mechanism of deep learning · Qingren Yao</title><meta name="description" content="A review on the attention mechanism of deep learning 读完这篇综述，对attention有了一些简单的了解，这里对内容和值得引文的文章进行了简单的总结。
attention机制是一个开支散叶非常多的领域，对其进行分类讨论非常的有必要。首先从人本身的"><meta name="keywords" content="Hexo,HTML,CSS,android,Linux"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">Qingren Yao</a></h3><div class="description"><p>Active Open-source Developer, Electric Information Master.</p></div></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://twitter.com/Qingren_2000"><i class="fa fa-twitter fa-2x"></i></a></li><li><a target="_blank" rel="noopener" href="http://github.com/qingrenn"><i class="fa fa-github fa-2x"></i></a></li><li><a href="mailto:yaoqingrenrobin@gmail.com"><i class="fa fa-envelope fa-2x"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>@ Qingren Yao 2022</span></a></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/archives">Archive</a></li><li><a href="/about">About</a></li><li><a href="/timeline">Timeline</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>A review on the attention mechanism of deep learning</a></h3></div><div class="post-content"><h1 id="A-review-on-the-attention-mechanism-of-deep-learning"><a href="#A-review-on-the-attention-mechanism-of-deep-learning" class="headerlink" title="A review on the attention mechanism of deep learning"></a>A review on the attention mechanism of deep learning</h1><p> 读完这篇综述，对attention有了一些简单的了解，这里对内容和值得引文的文章进行了简单的总结。</p>
<p>attention机制是一个开支散叶非常多的领域，对其进行分类讨论非常的有必要。首先从人本身的注意利形式开始：</p>
<ul>
<li>Bottom-up：可以理解为潜在的注意力，例如：在嘈杂的对话声中，人们往往更可能听到声音最大的那一种，这种形式在DL中与max-pooling以及gating machanism较为相似。</li>
<li>Top-down：可以理解为聚焦的注意力，例如：人们主积极动地关注某一个类物体，这种形式往往在DL的特定任务任务中使用。</li>
</ul>
<p>而如果根据attention模型结构上的区别，主要可以划分为以下几类：</p>
<center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/Attention.png" width=50%></center>

<p>在分类总结各种不同结构的模型前，可以先看看attention模型的统一结构</p>
<span id="more"></span>
<h2 id="1-attention的统一结构"><a href="#1-attention的统一结构" class="headerlink" title="1. attention的统一结构"></a>1. attention的统一结构</h2><center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/QEQcLG.png" width=80%></center>

<p>Vaswani对attention有一个非常好的总结：attention机制是一种将query和Key-Value对映射至输出的结构，该结构通过组合key和对应的query计算出每一个value对应的权重，然后将value的加权和作为输出。</p>
<p>the atten- tion mechanism ‘‘can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is com- puted by a compatibility function of the query with the corre- sponding key。</p>
<p>attention可以分为两步，第一步是根据keys和query计算attention权重，对应于上图上方的分支，第二步则根据values和对应的weigth计算出context vector。</p>
<p>以<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural machine translation by jointly learning to align and translate</a>为例，作者首次提出了attention，并将其应用至翻译任务中，其中attention是作为Seq2Seq模型的一部分，其中Key和Value相同，是各个时刻状态下encoder的输出$h_j$，query是decoder上一时刻的状态向量$s_{i-1}$，然后计算出权重$\alpha$​。</p>
<center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/截屏2021-08-06 下午3.54.39.png" width=25%></center>

<p>其中decoder的状态向量$s_i = f(s_{i-1}, y_{i-1}, c_i)$​​，（需要熟悉Seq2Seq）,下面则是本文中使用的attention表达式：</p>
<p>其中contex vector $c_i = \sum\limits_{j=1}^{T_x} \alpha_{ij}h_j$</p>
<p>其中权重$\alpha_{ij} = \frac{exp(e_{ij})}{\sum^{Tx}_{k=1}exp(e_{ik})}$ ，Score function为$e_{ij} = a(s_{i-1}, h_j)$​​</p>
<p>在attention中整合Keys和Query计算出Energy score的Score function方法有很多，在不同场景下各有优势：</p>
<center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/7PglGz.png"></center>

<h2 id="2-Forms-of-Feature-Sampling"><a href="#2-Forms-of-Feature-Sampling" class="headerlink" title="2. Forms of Feature Sampling"></a>2. Forms of Feature Sampling</h2><h3 id="Soft-attention"><a href="#Soft-attention" class="headerlink" title="Soft-attention"></a>Soft-attention</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural machine translation by jointly learning to align and translate</a>中使用的就是soft-attention，在计算contex vector时，使用的是values加权平均的方式，这样整个attention模块相对于输入是可微的（因为仅仅只涉及Key，Value， Query的四则运算），可以通过反向传播的方法进行训练。</p>
<h3 id="Hard-attention"><a href="#Hard-attention" class="headerlink" title="Hard-attention"></a>Hard-attention</h3><p><a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v37/xuc15.html">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a>在看图说话的任务上应用了attention，称之为hard-attention。因为还没有读过这篇文章，结合一篇<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52958865">知乎上的专栏</a>谈谈我的理解，专栏中以<a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?oi=gsb95&amp;q=Recurrent%20models%20of%20visual%20attention&amp;lookup=0&amp;hl=zh-CN">Recurrent Models of Visual Attention</a>为例。</p>
<center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/截屏2021-08-06 下午6.38.55.png" width=80%></center>

<p>图 B)结构显示了图片局部区域的学习，其输入为采集的整个场景图像的局部信息和第$l_{t-1}$​的RNN隐藏层向量，$l_{t-1}$​指导着Agent下一时刻的行动。Agent的局部信息采集的结构见图 A)部分，主要有某一点为中心的三个局部区域大小的图像采集，而不是采集整张图像。</p>
<p>Agent通过强化学习在每一个时刻做出决策，选取完整图像的局部信息，这个寻找图像前景局部区域的过程，可以视为Hard Attention的过程。</p>
<p>总之，Soft-attention通过计算权重来求Values的加权和，而Hard-attention通过强化学习的决策过程（可以视作一种离散分布）来求Values的组合。</p>
<h3 id="Global-attention-Local-attention"><a href="#Global-attention-Local-attention" class="headerlink" title="Global-attention/Local-attention"></a>Global-attention/Local-attention</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">Effective approaches to attention-based neural machine translation</a>在机器翻译上提出了global-attention和local-attention。</p>
<p>global-attention和soft-attention的整体思想一样，但在两篇文章中也存在一些差异。</p>
<p>local-attention则如其名，并不是对所有的Values都加权求和，在该文章中，作者设计了一个窗口，仅仅关注窗口中的隐藏层向量。</p>
<table>
<tr>
  <td align="center"><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/cBydmq.png" width=80%></td>
  <td align="center"><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/8jQvHv.png" width=80%></td>
</tr>
</table>

<h2 id="3-Forms-of-Input-Feature"><a href="#3-Forms-of-Input-Feature" class="headerlink" title="3. Forms of Input Feature"></a>3. Forms of Input Feature</h2><h3 id="Item-wise"><a href="#Item-wise" class="headerlink" title="Item-wise"></a>Item-wise</h3><p>Item-wise要求输入的是逐项的序列，例如：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural machine translation by jointly learning to align and translate</a>，Key和Value是一个时间序列，每一项对应encoder的隐层状态。</li>
<li><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper">Squeeze-and-Excitation Networks</a>，Key和Value是逐通道的，每一项对应一个通道。</li>
</ul>
<center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/xPXCci.png"></center>

<h3 id="Location-wise"><a href="#Location-wise" class="headerlink" title="Location-wise"></a>Location-wise</h3><p>location-wise attention针对的是难以获得不同输入项的任务，通常这种注意力机制用于视觉任务。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?oi=gsb95&amp;q=Recurrent%20models%20of%20visual%20attention&amp;lookup=0&amp;hl=zh-CN">Recurrent Models of Visual Attention</a>中的Value是特征图某一个局部块。</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.7755">Multiple object recognition with visual attention</a>，没有详细了解，不过看样子和上一篇差不多。</li>
</ul>
<h2 id="4-Input-Representation"><a href="#4-Input-Representation" class="headerlink" title="4. Input Representation"></a>4. Input Representation</h2><h3 id="distinctive-attention"><a href="#distinctive-attention" class="headerlink" title="distinctive attention"></a>distinctive attention</h3><p>大多数attention模型都包涵两个特征：</p>
<ul>
<li>模型包括一个单一的输入和相应的输出序列。</li>
<li>Keys和Queries是独立的序列。</li>
</ul>
<p>这样的attention模型也可称之为distinctive attention，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.02874">An Attentive Survey of Attention Models</a>。</p>
<h3 id="co-attention"><a href="#co-attention" class="headerlink" title="co-attention"></a>co-attention</h3><p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2016/hash/9dcb88e0137649590b755372b040afad-Abstract.html">Hierarchical question-image co-attention for visual question answering</a>提出的多输入的attention模型</p>
<h3 id="Muti-grained-coarse-grained-fine-grained"><a href="#Muti-grained-coarse-grained-fine-grained" class="headerlink" title="Muti-grained(coarse-grained/fine-grained)"></a>Muti-grained(coarse-grained/fine-grained)</h3><p><a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Multi-grained+attention+network+for+aspect-level+sentiment+classification&amp;btnG=">Multi-grained attention network for aspect-level sentiment classification</a></p>
<h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p><a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?oi=gsb95&amp;q=%20Inner%20attention%20based%20recurrent%20neural%20networks%20for%20answer%20selection&amp;lookup=0&amp;hl=zh-CN"> Inner attention based recurrent neural networks for answer selection</a></p>
<p>Self-attention中key，query，value是同一个序列的不同表示，这样的结构被使用在了<a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?oi=gsb95&amp;q=attention%20is&amp;lookup=0&amp;hl=zh-CN">Tranformer</a>中。</p>
<h3 id="hierachical-attention"><a href="#hierachical-attention" class="headerlink" title="hierachical attention"></a>hierachical attention</h3><center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/dKjpUR.png"></center>

<p><a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?oi=gsb95&amp;q=Hierarchical%20attention%20networks%20for%20document%20classification%2C%20&amp;lookup=0&amp;hl=zh-CN">Hierarchical attention networks for document classification</a></p>
<p>CV方向上的应用：<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2015/html/Xiao_The_Application_of_2015_CVPR_paper.html">The Application of Two-Level Attention Models in Deep Convolutional Neural Network for Fine-Grained Image Classification</a></p>
<h2 id="5-Output-Representation"><a href="#5-Output-Representation" class="headerlink" title="5. Output Representation"></a>5. Output Representation</h2><h3 id="Muti-head"><a href="#Muti-head" class="headerlink" title="Muti-head"></a>Muti-head</h3><p><a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?oi=gsb95&amp;q=attention%20is&amp;lookup=0&amp;hl=zh-CN">Attention is all you need</a></p>
<h3 id="Muti-dimensional"><a href="#Muti-dimensional" class="headerlink" title="Muti-dimensional"></a>Muti-dimensional</h3><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/11941">DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding</a></p>
<center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/KR1zzr.png" wdith=80%></center>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2021-08-07</span><i class="fa fa-tag"></i><a class="tag" href="/categories/学习总结/" title="学习总结">学习总结 </a><a class="tag" href="/tags/深度学习/" title="深度学习">深度学习 </a><a class="tag" href="/tags/attention综述/" title="attention综述">attention综述 </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://qingrenn.github.io/2021/08/07/A review on the attention mechanism of deep learning/,Qingren Yao,A review on the attention mechanism of deep learning,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2021/08/20/Attention%20is%20all%20you%20need/" title="Attention is all you need">prev_post</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2021/08/03/Python%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" title="Python内置数据结构">next_post</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>