<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Qingren Yao"><title>Valse-2021 Hangzhou · Qingren Yao</title><meta name="description" content="VALSE 2021第一次参加学术会议，记录一下在杭州的收获、和点滴。
不积小流无以成江海。




时间：10.08
如何突破机器学习的先验假设来自徐宗本院士

徐院士主要介绍了机器学习在使用是的五个先觉条件，这也是限制机器学习的五个方面的假设。在研究机器学习、深度学习的任务中时，可从这五个方面着"><meta name="keywords" content="Hexo,HTML,CSS,android,Linux"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">Qingren Yao</a></h3><div class="description"><p>Active Open-source Developer, Electric Info Master.</p></div></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://twitter.com/Qingren_2000"><i class="fa fa-twitter fa-2x"></i></a></li><li><a target="_blank" rel="noopener" href="http://github.com/qingrenn"><i class="fa fa-github fa-2x"></i></a></li><li><a href="mailto:yaoqingrenrobin@gmail.com"><i class="fa fa-envelope fa-2x"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>@ Qingren Yao 2022</span></a></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/archives">Archive</a></li><li><a href="/about">About</a></li><li><a href="/timeline">Timeline</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Valse-2021 Hangzhou</a></h3></div><div class="post-content"><h1 id="VALSE-2021"><a href="#VALSE-2021" class="headerlink" title="VALSE 2021"></a>VALSE 2021</h1><p>第一次参加学术会议，记录一下在杭州的收获、和点滴。</p>
<p>不积小流无以成江海。</p>
<center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/photo1.png></center>

<span id="more"></span>
<hr>
<p>时间：10.08</p>
<h2 id="如何突破机器学习的先验假设"><a href="#如何突破机器学习的先验假设" class="headerlink" title="如何突破机器学习的先验假设"></a>如何突破机器学习的先验假设</h2><center>来自徐宗本院士</center>

<p>徐院士主要介绍了机器学习在使用是的五个先觉条件，这也是限制机器学习的五个方面的假设。在研究机器学习、深度学习的任务中时，可从这五个方面着手，科学地（数学地）进行研究设计。</p>
<p>这五个先觉条件，个人认为是机器学习算法设计的五个方面（损失函数、模型结构/正则项涉及、数据分布、收敛性）。在数学的层面对其分析，对研究会更有帮助。</p>
<center><img src="https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/35CA7565-45EF-4A7D-A680-A8FDF090AF0A_1_201_a.jpeg" width=50%></center>

<ul>
<li>Independence Hypothesis （独立假设）</li>
<li>Large Capacity/Regularizer hypothesis（大容量假设/正则项假设）</li>
<li>Completeness Hypothesis（训练数据完备性假设）</li>
<li>Euclidean Hypothesis（欧式空间假设）</li>
</ul>
<p>CSDN上有完整的<a target="_blank" rel="noopener" href="https://blog.csdn.net/cf2SudS8x8F0v/article/details/119430157">报告内容</a></p>
<p>参考文献：</p>
<table><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/911DE4C2-1BA5-4AF3-8FF2-E0B9FDD5A28A_1_105_c.jpeg></td>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/F481CB9B-5810-48D8-A89B-ACC567BA9056_1_105_c.jpeg></td>
</tr>
<tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/FD3A1E46-5BB3-49A4-BC72-F5ED12C82618_1_105_c.jpeg ></td>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_0018.jpeg width=“20%”></td>
</tr>
</table>

<hr>
<p>时间：10.08</p>
<h2 id="动态神经网络综述"><a href="#动态神经网络综述" class="headerlink" title="动态神经网络综述"></a>动态神经网络综述</h2><center>来自黄高老师</center>

<p>动态网络最新综述：Dynamic Neural Networks: A Survey [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.04906">文献地址</a>]</p>
<table><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_0028.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/a.jpg></td>
</tr></table>

<hr>
<p>时间：10.08</p>
<h2 id="AI赋能高分遥感目标检测与识别：挑战、对策及展望"><a href="#AI赋能高分遥感目标检测与识别：挑战、对策及展望" class="headerlink" title="AI赋能高分遥感目标检测与识别：挑战、对策及展望"></a>AI赋能高分遥感目标检测与识别：挑战、对策及展望</h2><center>来自韩军伟（西北工业大学）</center>

<table><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_AE83B3E7BA4F-1.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_6E91F8817B72-1.jpeg></td>
</tr></table>

<p>1.旋转不变形</p>
<p>Cheng G, Zhou P, Han J. Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images[J]. IEEE Transactions on Geoscience and Remote Sensing, 2016, 54(12): 7405-7415.</p>
<p>Cheng G, Han J, Zhou P, et al. Learning rotation-invariant and fisher discriminative convolutional neural networks for object detection[J]. IEEE Transactions on Image Processing, 2018, 28(1): 265-278.</p>
<p>2.有向目标检测</p>
<p>Xie X, Cheng G, Wang J, et al. Oriented R-CNN for Object Detection[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 3520-3529.</p>
<p>Cheng G, Wang J, Li K, et al. Anchor-free Oriented Proposal Generator for Object Detection[J]. arXiv preprint arXiv:2110.01931, 2021.</p>
<center class=half>
  <img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_759B298932F9-1.jpeg width=50%><imf src=><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_7E7AAA9A1658-1.jpeg width=50%>
</center>

<p>3.弱监督学习</p>
<p>4.小样本</p>
<p>Cheng G, Cai L, Lang C, et al. SPNet: Siamese-Prototype Network for Few-Shot Remote Sensing Image Scene Classification[J]. IEEE Transactions on Geoscience and Remote Sensing, 2021.</p>
<center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_2E88DA82ED98-1.jpeg width=50%></center>

<p>5.细粒度</p>
<p>Han J, Yao X, Cheng G, et al. P-CNN: Part-based convolutional neural networks for fine-grained visual categorization[J]. IEEE transactions on pattern analysis and machine intelligence, 2019.</p>
<p>6.研究背景及展望</p>
<table><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_C8D11EB186C4-1.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_58182C115C94-1.jpeg></td>
</tr></table>

<ul>
<li>快速、轻量、大幅度画面</li>
<li>弱小目标检测</li>
<li>域自适应</li>
</ul>
<hr>
<p>时间：10.09</p>
<h2 id="模式识别：从分类到理解"><a href="#模式识别：从分类到理解" class="headerlink" title="模式识别：从分类到理解"></a>模式识别：从分类到理解</h2><center>来自刘成林（中国科学院自动化研究所）</center>

<table><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_76E0BCF7DD30-1.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_5CBD07D3A7FD-1.jpeg></td>
</tr><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_FB77EC8010F2-1.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_E340D62DCD9F-1.jpeg></td>
</tr>
</table>

<hr>
<p>时间：10.09</p>
<h2 id="视频理解"><a href="#视频理解" class="headerlink" title="视频理解"></a>视频理解</h2><center>来自杨易（浙江大学）</center>

<p>具体不是很懂</p>
<p>不过发现Transformer在视频领域也得到了应用。</p>
<p>Arnab A, Dehghani M, Heigold G, et al. Vivit: A video vision transformer[J]. arXiv preprint arXiv:2103.15691, 2021.</p>
<hr>
<p>时间：10.09</p>
<h2 id="分布外-Out-Of-Distribution-泛化"><a href="#分布外-Out-Of-Distribution-泛化" class="headerlink" title="分布外(Out-Of-Distribution)泛化"></a>分布外(Out-Of-Distribution)泛化</h2><center>崔鹏（清华大学）</center>

<center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/SL4LTa.png></center>

<p>Shen Z, Liu J, He Y, et al. Towards Out-Of-Distribution Generalization: A Survey[J]. arXiv preprint arXiv:2108.13624, 2021.[<a target="_blank" rel="noopener" href="http://pengcui.thumedialab.com/papers/OOD_APR_valse2021.pdf">PPT</a>]</p>
<p>此外崔老师还做了一个因果推断的Tutorial[<a target="_blank" rel="noopener" href="http://pengcui.thumedialab.com/papers/Stable%20Learning-tutorial-valse2021.pdf">PPT</a>]</p>
<hr>
<p>时间：10.09</p>
<h2 id="对抗视觉生成与合成年度进展概述"><a href="#对抗视觉生成与合成年度进展概述" class="headerlink" title="对抗视觉生成与合成年度进展概述"></a>对抗视觉生成与合成年度进展概述</h2><center>来自谭明奎（华南理工大学）</center>

<p>关于GAN的发展历程</p>
<center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_6D413FB21E1E-1.jpeg></center>

<p><strong>Pixel2style2pixel</strong></p>
<p>Richardson E, Alaluf Y, Patashnik O, et al. Encoding in style: a stylegan encoder for image-to-image translation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 2287-2296.</p>
<table><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_07105D8935B1-1.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_D6F15CB15ABC-1.jpeg></td>
</tr></table>

<hr>
<p>时间：10.09</p>
<h2 id="基于物理模型的计算光谱摄像"><a href="#基于物理模型的计算光谱摄像" class="headerlink" title="基于物理模型的计算光谱摄像"></a>基于物理模型的计算光谱摄像</h2><center>来自黄华（北京师范大学）</center>

<p>正向成像模型</p>
<p>$Y=H(X)+N$</p>
<p>计算重构模型</p>
<p>$\hat{X}=argmin \left | Y-H(X)\right |^{2}_{2}+\lambda\Phi(X)$</p>
<center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/EBD52DB8-3428-4E47-B5CC-C2BAC7CB69BC.jpeg width="80%"></center>

<p>要构建基本模型+数据驱动的深度学习方法</p>
<table><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/EBD52DB8-3428-4E47-B5CC-C2BAC7CB69BC.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_1B7432445A88-1.jpeg></td>
</tr><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_55023C1EF73D-1.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_2CF30BE0A5A1-1.jpeg></td>
</tr>
</table>

<hr>
<p>时间：10.10</p>
<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><center>来自黄雷 （北京航空航天大学）</center>

<p>之前从未关注过，有关Normalization的研究。作为深度学习模型中重要的一环，值得花时间深入理解一下。</p>
<ul>
<li>什么是Normalization？</li>
<li>使用Normlization的动机是什么？</li>
</ul>
<table><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_2FE2DDA897FE-1.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_3FC8475793C0-1.jpeg></td>
</tr><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_3FC4BE6AEABE-1.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_FE6B266160B2-1.jpeg></td>
</tr>
</table>

<hr>
<p>时间：10.10</p>
<h2 id="全生命周期的自动化机器学习算法研究和行业实践"><a href="#全生命周期的自动化机器学习算法研究和行业实践" class="headerlink" title="全生命周期的自动化机器学习算法研究和行业实践"></a>全生命周期的自动化机器学习算法研究和行业实践</h2><center>来自孙明（快手科技）</center>
AutoML的全周期研究，从数据集、数据增强到网络结构、损失函数、优化器...

<center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_F3D1BA0F4F3E-1.jpeg width="80%"></center>

<hr>
<p>时间：10.10</p>
<h2 id="视觉基础模型架构设计新思路"><a href="#视觉基础模型架构设计新思路" class="headerlink" title="视觉基础模型架构设计新思路"></a>视觉基础模型架构设计新思路</h2><center>来自张祥雨 （旷视科技）</center>

<p>不得不佩服大佬的思路，大佬是怎么思考ViT的。</p>
<center><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_E43A751B92C6-1.jpeg width=“80%”></center>

<p>其中关键在与大感受野（大卷积核）Muti-head Self-Attention不是必须的。</p>
<table><tr>
<td width="50%"><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_015FCD4C0C9B-1.jpeg></td>
<td width=“50%”><img src=https://cdn.jsdelivr.net/gh/Qingrenn/FigureBed@master/uPic/IMG_C1BA0FA318FC-1.jpeg></td>
</tr>
</table></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2021-10-13</span><i class="fa fa-tag"></i><a class="tag" href="/categories/学习总结/" title="学习总结">学习总结 </a><a class="tag" href="/tags/深度学习/" title="深度学习">深度学习 </a><a class="tag" href="/tags/学术会议/" title="学术会议">学术会议 </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://qingrenn.github.io/2021/10/13/VALSE-2021/,Qingren Yao,Valse-2021 Hangzhou,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2021/12/08/ViTVariants/" title="学术分享：ViT &amp; Its Variants">prev_post</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2021/10/04/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/" title="CV入门学习路线">next_post</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>